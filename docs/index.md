---
layout: default
---

In the past decade, data-driven analysis has emerged as a growing methodology, if not sub-discipline within literary studies. This approach, broadly described as “_distant reading_”, has harnessed available technology to open new avenues for how we understand literary texts, both individually and in the aggregate. Whereas traditional literary scholarship is generally grounded in the interpretation of the specific language of a text or body of texts, macroanalytic approaches have offered new ways of seeing texts. Rather than generating claims based upon the interpretation of a handful of novels, distant reading offers the opportunity to work with hundreds, if not thousands, of texts; this has led to analyses, for example, of the nature of and changes within semantic fields across the span of decades, or the identification of generic markers within large corpora of texts. However, while computational analysis provides the opportunity to pursue previously unimaginable questions, scholars are still in the early stages of establishing the significance and meaning of such approaches, particularly in relation to the inherited body of scholarship that defines the discipline. What, in other words, do the graphs, charts, and maps generated by data-driven analysis tell us about literature, and about what an individual text might mean? Thus, in addition to pursuing new questions and applying new methods in the computational analysis of literary texts, there is a continuing need to theorise the relationship between macroanalytic and microanalytic (distant and close) readings of individual works.

This project brings together researchers from computer science and literary studies to develop and evaluate novel distant reading approaches.

## [](#header-2-1)Project outcome I: Creating Transcendental Information Cascades from English Literature

[![Alt Screencast demonstration of the distant reading tool for English literature](http://img.youtube.com/vi/QnXvtYlL9VA/0.jpg)](http://www.youtube.com/watch?v=QnXvtYlL9VA)

Our method applies the Transcendental Information Cascades (TIC) approach to understand how emergent structures of information are generated during the unfolding of a text. It treats the text as a diachronically evolving information system and uses TIC to isolate the structural properties of that system. Theoretically, one could create an infinite number of models of any given system, depending on the dictionary one chose as the structuring element of the system (e.g., bigrams or trigrams, place names, articles or prepositions, affect words, etc.). We use character names as the structuring element. The network thus provides a visualization of the occurrence of characters and models the information structures they generate.

Our approach treats character names as units of information; thus it is not concerned primarily with their social relation, though a social network can be extrapolated from the dynamic network. The approach, instead, allows analysis of how a cast of characters is generated and managed dynamically over the duration of a text. The text is broken into “slices” of 1000 words (with chapter breaks terminating a slice). Dickens’s larger novels are over 300,000 words, with chapters averaging 4,000 and 8,000 words; this size produces a complex but still manageable network—larger or smaller slices would produce different networks with different properties. Each slice is scanned to detect the presence of the character, so each node in the network is defined by the characters who appear in that segment of the text. Two characters can appear in the same node if they are not linked explicitly or relationally in that segment of text (i.e., they are linked by the contiguity of their names in the text rather than by their interaction), and a character can appear in a node if they do not physically appear in the represented action (e.g., another character mentions or thinks of that character). Edges signify consecutive appearances of a character.

![alt Network construction](https://github.com/vuw-sim-stia/computational-literary-science/raw/master/docs/net1.png "Network construction")

 
The algorithm works from the input of two files: (1) a file containing the text of a novel and (2) a library of character names. Text files for the novels were taken from Project Gutenberg; character libraries were generated beginning with resources such as Pierce (1878) and Hawes (1998), though these lists had to be refined manually so that the “tags” for each character flagged all appearances of that character. For example, the list of tags for Bella Wilfer in Our Mutual Friend consisted of six identifiers: “Bella Wilfer, Miss Wilfer, Bella, Mrs John Harmon, Mrs John Rokesmith, Mrs Rokesmith.” The process and algorithm, as they were refined, produced a high degree of accuracy; full accuracy (i.e., the matching of every instance of a character appearance to the correct character) cannot be guaranteed. In some instances, the text of the novel itself was modified so as to ensure accuracy. Martin Chuzzlewit, for instance, contains two characters named Martin Chuzzlewit, so instances of “Martin” and “Mr. Chuzzlewit” had to be manually disambiguated so that the correct character was flagged. The algorithm can be run on any text, provided the user can input the text file and the character library. Texts for hundreds (even thousands) of novels not under copyright are readily available through resources like Project Gutenberg, though character lists need to be created manually—this presents the most significant barrier to the large-scale application of the method. Character lists need not be comprehensive, as a network can represent any selection of characters from a given novel (e.g., only major characters, only female characters, characters with a particular type of relationship, etc.). 


## [](#header-2-2)Team

Led by Markus Luczak-Roesch and Adam Grener this interdisciplinary research project was supported under the [spearheading digital futures theme](http://www.victoria.ac.nz/about/strengths/digital-futures), which is part of the areas of strategic distictiveness of Vicotria University of Wellington. 

### [](#header-3-1)Markus Luczak-Roesch

Markus Luczak-Roesch is a Senior Lecturer in Information Systems at the School for Information Management, Victoria Business School, Victoria University of Wellington. Before joining Victoria Markus worked as a Senior Research Fellow on the prestigious EPSRC programme grant [SOCIAM - The Theory and Practice of Social Machines](http://sociam.org) at the University of Southampton, Electronics and Computer Science (UK, 2013-2016). A computer scientist by education, Markus investigates the formal properties of information in socio-technical systems and human factors of information and computing systems. More information: [http://markus-luczak.de](http://markus-luczak.de)

### [](#header-3-2)Adam Grener

Adam Grener is Lecturer in the English Programme at Victoria University of Wellington. His main area of research is the nineteenth-century British novel, though he also has interest in the history of the novel, narrative theory, and computational approaches to literature. His work has appeared in the journals Genre, Narrative, and Modern Philology, and he is the co-editor of a special issue of Genre, "Narrative Against Data in the Victorian Novel," set to appear in March 2017. He is completing a book on realist aesthetics and the history of probabilistic thought. More information: [http://www.victoria.ac.nz/seftms/about/staff/adam-grener](http://www.victoria.ac.nz/seftms/about/staff/adam-grener)

### [](#header-3-3)Research assistants
 * Emma Fenton
 * Tom Goldfinch
